{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let start with a simple Pipeline that uses Multinomial Naive Bayes Classifier\n",
    "#CountVectorizer will tokenize our sentences and build a dicitonary of features and transform doc to feature vectors\n",
    "#Defualt is N-grams is one. We will deal with this later\n",
    "#“Term Frequency times Inverse Document Frequency” tf–idf  divides the number of occurrences of each word in a document\n",
    "# by the total number of words in the document\n",
    "\n",
    "#tokenizer=tokenize made a lttle impact at all  0.88 to 0.89 Positive. It is slow, so not worth it\n",
    "\n",
    "mnb_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', MultinomialNB()),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets train a model with Multinomial Naive Bayes Classifier\n",
    "_ = mnb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Curious Let check ...\n",
    "simple_test = ['God og hurtig hjælp', 'Jeg fik en super god og hurtig behandling af min sag',\n",
    "               'Det er mildest talt utrygt som potentiel kommende kunde, at jeres medarbejdere'+\\\n",
    "               ' ikke er i stand til at udfylde basale oplysninger på en police']\n",
    "#The first two are positive while the last is negative. So we should have 1, 1, 0\n",
    "mnb_clf.predict(simple_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_text = ['jeg elsker mad','det er dårlig mad','det er ikke dårlig mad','det er ok','det er ikke ok','farvel']+simple_test\n",
    "\n",
    "print('Problem with negatition is clear in 4nd and 5rd')\n",
    "for i in mnb_clf.predict(sent_text):\n",
    "    print([name for name,score in target_name.items() if score == i][0],':')\n",
    "    print('\\t', sent_text.pop(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is our model score?\n",
    "mnb_clf.score(X_test, y_test) #another way is: np.mean(mnb_clf.predict(X_test)==y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, mnb_clf.predict(X_test),target_names=['Negative','Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The best thing about Scikit-learn is inuitive easy design which allow you to change models without chaning much code\n",
    "#change 'hinge' to 'los' to get predict_probaility\n",
    "sgd_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, n_iter=5, random_state=42)),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How Good is Supported Vector Machine Classiffier\n",
    "_ = sgd_clf.fit(X_train, y_train)\n",
    "np.mean(sgd_clf.predict(X_test) == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, sgd_clf.predict(X_test),target_names=['Negative','Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf2 = Pipeline([('vect', CountVectorizer()),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss ='log' ,penalty='l2',alpha=1e-3, n_iter=5, random_state=42)),]) #loss='hinge', \n",
    "_ = sgd_clf2.fit(X_train, y_train)\n",
    "np.mean(sgd_clf2.predict(X_test) == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Grid Search CV(cross validation) we can find best vect ngram, tdidf_use and clf_alpha in SGD model\n",
    "#n_jobs = -1 means using all cores power :)\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1,1),(1, 2)],\n",
    "               'tfidf__use_idf': (True, False),\n",
    "               'clf__alpha': (1e-2, 1e-3),}\n",
    "gsSGD_clf = GridSearchCV(sgd_clf, parameters, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will take a while\n",
    "_ = gsSGD_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsSGD_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsSGD_clf.predict(['ikke'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, gsSGD_clf.predict(X_test),target_names=['Negative','Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf  = Pipeline((\n",
    "        ('vec',  CountVectorizer(ngram_range=(1, 2), max_features=100000)),\n",
    "        ('fe_se', SelectKBest(chi2, k=5000)),\n",
    "        ('clf', LogisticRegressionCV(n_jobs=4))\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes a while to train :)\n",
    "_= lr_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, lr_clf.predict(X_test),target_names=['Negative','Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function modification of Mike Lee Williams(mike@mike.place). All credit to Mike\n",
    "\n",
    "feature_names = lr_clf.steps[0][1].get_feature_names()\n",
    "feature_names = [feature_names[i] for i in \n",
    "                 lr_clf.steps[1][1].get_support(indices=True)]\n",
    "\n",
    "def show_most_informative_features(feature_names, clf, n=1000):\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % ((coef_1), fn_1, (coef_2), fn_2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
